{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNOn16wA9tyKLcXpjAr/D7i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rmdnps10/23-1-Nlp-Project/blob/main/Project_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 프로젝트 Pipeline"
      ],
      "metadata": {
        "id": "IsLHFTbBfdb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('sentiwordnet')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "id": "k9cDkG9Zidsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "iJUDA4PwJcgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install xformers"
      ],
      "metadata": {
        "id": "1qgAVEjlLPi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. 수민이가 크롤링한 텍스트 전처리\n",
        "- 현대, 기아도 여기다 넣으면돼요"
      ],
      "metadata": {
        "id": "pZ_B2nbYiScV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file_path = 'Hyundai_reddit_nogada.csv'"
      ],
      "metadata": {
        "id": "F-FZw0mbQwl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "samsung_list = []\n",
        "with open(csv_file_path, 'r', encoding='utf-8') as file:\n",
        "    csv_reader = csv.reader(file)\n",
        "    for row in csv_reader:\n",
        "        row_string = ','.join(row)\n",
        "        samsung_list.append(row_string)\n",
        "\n",
        "print(samsung_list)\n"
      ],
      "metadata": {
        "id": "pgg8a5fwiOS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_special_characters(text):\n",
        "    clean = re.sub('[^a-zA-Z0-9\\s]', '', text)\n",
        "    return clean\n",
        "\n",
        "clean_list= []\n",
        "\n",
        "for text in samsung_list:\n",
        "  clean = remove_special_characters(text)\n",
        "  clean_list.append(clean)\n",
        "\n",
        "print(clean_list)\n"
      ],
      "metadata": {
        "id": "xIh773hZh2w1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unify_text(text):\n",
        "    unify = text.lower()\n",
        "    return unify\n",
        "\n",
        "unified_text = []\n",
        "\n",
        "for text in samsung_list:\n",
        "  unify = unify_text(text)\n",
        "  unified_text.append(unify)\n",
        "\n",
        "print(unified_text)\n"
      ],
      "metadata": {
        "id": "HOmLyZBqi6_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-1. 키워드 추출 -> Term-Frequency"
      ],
      "metadata": {
        "id": "_zw-bQfKfm5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 가장 많이 나온 명사 TOP50"
      ],
      "metadata": {
        "id": "9k8BkjaCk8mZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "def find_most_frequent_nouns(string_list, top_n=20):\n",
        "    words = [word for string in string_list for word in word_tokenize(string)]\n",
        "    tagged_words = pos_tag(words)\n",
        "    nouns = [word for word, pos in tagged_words if pos.startswith('NN')]\n",
        "    noun_counts = Counter(nouns)\n",
        "    most_common_nouns = noun_counts.most_common(top_n)\n",
        "    return most_common_nouns\n",
        "\n",
        "most_frequent_nouns = find_most_frequent_nouns(unified_text, top_n=50)\n",
        "for noun, count in most_frequent_nouns:\n",
        "    print(noun, count)"
      ],
      "metadata": {
        "id": "GQrGvbI9flRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- word cloud 생성 (코퍼스에서 명사만 추출해서 단어빈도수에 따라 클라우드 구성))"
      ],
      "metadata": {
        "id": "oR4Zv-21kzMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# 명사 추출 함수\n",
        "def extract_nouns(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    nouns = [token for token in tokens if token.isalpha() and token not in stopwords.words('english')]\n",
        "    return nouns\n",
        "\n",
        "# 텍스트 데이터셋에서 명사 추출\n",
        "nouns_list = []\n",
        "for text in unified_text:\n",
        "    nouns = extract_nouns(text)\n",
        "    nouns_list.extend(nouns)\n",
        "\n",
        "# 명사 리스트를 공백으로 연결하여 하나의 문자열로 만듦\n",
        "nouns_text = ' '.join(nouns_list)\n",
        "\n",
        "# WordCloud 생성\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(nouns_text)\n",
        "\n",
        "# WordCloud 시각화\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "f2bKXlsNZGV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "삼성 워드 클라우드 다운로드 코드"
      ],
      "metadata": {
        "id": "9GXakZX_lRbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud.to_file('hyundai_wordcloud.png')"
      ],
      "metadata": {
        "id": "vEfMha7QlOCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-2. 키워드 추출 -> Top2vec"
      ],
      "metadata": {
        "id": "950Qfz3XlnyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install top2vec"
      ],
      "metadata": {
        "id": "1dPSbqi8ltAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from top2vec import Top2Vec\n",
        "\n",
        "model = Top2Vec(samsung_list)"
      ],
      "metadata": {
        "id": "UGlbPxCnmHXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " model.get_num_topics()"
      ],
      "metadata": {
        "id": "wZbn06CvnU35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_words, word_scores, topic_scores, topic_nums = model.search_topics(keywords=[\"hyundai\"], num_topics=2)\n",
        "for topic in topic_nums:\n",
        "    model.generate_topic_wordcloud(topic)\n"
      ],
      "metadata": {
        "id": "uyFzGgTtm_js"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. 주요 키워드에 대한 Sentiment Analysis\n",
        "\n"
      ],
      "metadata": {
        "id": "KxTVDXyjn8ew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 문장에 대하여 sentiment analysis을 통하여 감정 점수를 부여하여,일정 임계점 이상의 감정 점수가 부여된 문장만 필터링"
      ],
      "metadata": {
        "id": "CaaySCE7RtKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import sentiwordnet as swn\n",
        "\n",
        "# 문자열 리스트\n",
        "\n",
        "# 감정적인 단어가 있는지 확인하여 삭제할 문자열을 담을 리스트\n",
        "filtered_list = []\n",
        "\n",
        "# 각 문자열에 대해 감정적인 단어 여부를 확인하고 삭제할 문자열을 filtered_list에 추가\n",
        "for sentence in unified_text:\n",
        "    words = sentence.split()  # 문자열을 단어로 분리\n",
        "\n",
        "    # 각 단어에 대해 감정적인 단어 여부 확인\n",
        "    has_emotional_word = False\n",
        "    for word in words:\n",
        "        synsets = list(swn.senti_synsets(word))\n",
        "        if synsets:\n",
        "            has_emotional_word = True\n",
        "            break\n",
        "\n",
        "    if has_emotional_word:\n",
        "        filtered_list.append(sentence)\n",
        "\n",
        "print(filtered_list)\n",
        "len(filtered_list)\n",
        "\n"
      ],
      "metadata": {
        "id": "Xn1JyKYFluv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 삼성과 관련된 주요 키워드를 포함하고 있는 문장만 필터링 ( 광고성, 아예 주제랑 벗어나는 말 제외하기 위함))"
      ],
      "metadata": {
        "id": "7cHBbgaqECl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 문자열 리스트\n",
        "\n",
        "\n",
        "# 원하는 키워드 리스트\n",
        "##desired_keywords = [\"samsung\", \"galaxy\",\"s23\",\"s22\",\"s23u\",\"ultra\",\"apple\",\"pay\",\"oneui\",\"s21\",\"s22u\",\"s23u\",\"ultra\", \"screen\", \"camera\", \"battery\",\"service\"]\n",
        "desired_keywords= [\"hyundai\", \"sonata\",\"elantra\", \"car\", \"engine\", \"dealer\",\"oil\",\"dealership\", \"service\", \"warranty\"]\n",
        "# 원하는 키워드가 없는 경우 리스트 요소를 제거\n",
        "keyword_list = [sentence for sentence in filtered_list if any(keyword in sentence for keyword in desired_keywords)]\n",
        "\n",
        "print(keyword_list)\n",
        "len(keyword_list)\n"
      ],
      "metadata": {
        "id": "7gjOyQcynx1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2차 필터링 후 문장 단위로 리스트를 만들어주기."
      ],
      "metadata": {
        "id": "Uq-g4ZJPt93m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def index_sentences(string_list):\n",
        "    indexed_list = []\n",
        "    for string in string_list:\n",
        "        sentence_list = []\n",
        "        temp_sentence = ''\n",
        "        for char in string:\n",
        "            temp_sentence += char\n",
        "            if char in ['.', '?', '!']:\n",
        "                sentence_list.append(temp_sentence.strip())\n",
        "                temp_sentence = ''\n",
        "        if temp_sentence:\n",
        "            sentence_list.append(temp_sentence.strip())\n",
        "        indexed_list.extend(sentence_list)\n",
        "    return indexed_list\n",
        "\n",
        "\n",
        "indexed_list = index_sentences(keyword_list)\n",
        "print(indexed_list)\n",
        "len(indexed_list)"
      ],
      "metadata": {
        "id": "sv_B9pZht9Jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3차 필터링: \"배터리\"** 가 포함된 문장만 뽑아내서, 본격적인 분석\n",
        "\n",
        "## **여기서부터 아래 코드의 경우, desired_keywords만 바꿔줘서 반복하여 코드돌리면됨**\n",
        "\n",
        "- 키워드에 대한 긍/부정 csv 파일 다운로드\n",
        "- 키워드에 대한 긍/부정 wordcloud 다운로드\n",
        "\n",
        "<삼성의 경우>\n",
        "\n",
        "Desired keywords =\n",
        "1. [\"s23\",\"flip\",\"fold\",\"s22\",\"note\"]\n",
        "2. [\"battery\"]\n",
        "3. [\"camera\"]\n",
        "4. [\"software\",\"oneui\"]\n",
        "5. [\"screen\"]\n",
        "\n",
        "\n",
        "<현대의 경우>\n",
        "\n",
        "Desired keywords =\n",
        "1. [\"sonata\"]\n",
        "2. [\"elantra\"]\n",
        "3. [\"engine\"]\n",
        "4. [\"oil\"]\n",
        "4. [\"dealer\",\"customer service\"]\n",
        "5. [\"design\"]"
      ],
      "metadata": {
        "id": "RFNV5xqIDKuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 원하는 키워드 입력\n",
        "desired_keywords = [\"design\"]"
      ],
      "metadata": {
        "id": "duubBxGcE8Li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 키워드 포함된 문장 모두 뽑아냅니다!"
      ],
      "metadata": {
        "id": "ywsSqK7wNdhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "battery_list = [sentence for sentence in indexed_list if any(keyword in sentence for keyword in desired_keywords)]\n",
        "\n",
        "\n",
        "print(battery_list)\n",
        "print(len(battery_list))"
      ],
      "metadata": {
        "id": "SQ64SanktlVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Huggin Face의 sentiment analysis model 이용해서 긍/부정 판별"
      ],
      "metadata": {
        "id": "mcZGOJESNlP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# 감정 분석 모델 로드\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=model_name)\n",
        "\n",
        "# 분류할 문장들\n",
        "\n",
        "sentences= battery_list\n",
        "\n",
        "# 문장 분류\n",
        "positive_list = []\n",
        "negative_list = []\n",
        "\n",
        "for sentence in sentences:\n",
        "    result = sentiment_analyzer(sentence)[0]\n",
        "    label = result['label']\n",
        "\n",
        "    if label == \"POSITIVE\":\n",
        "        positive_list.append(sentence)\n",
        "    elif label == \"NEGATIVE\":\n",
        "        negative_list.append(sentence)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"Positive List:\", positive_list)\n",
        "print(\"Negative List:\", negative_list)\n"
      ],
      "metadata": {
        "id": "02EljEWMI9n2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### 키워드를 포함하고 있는 문장에 대한 긍정, 부정의 비율을 통한 점수 주고 키워드의 긍정/부정 코퍼스 csv파일로 다운로드 하는 코드\n",
        " -> 이거 일일이 보면서 인사이트 추출"
      ],
      "metadata": {
        "id": "2bTuCpVDEWU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print('score: {0:.2f}'.format(len(positive_list)/ (len(positive_list) + len(negative_list)) *100) )\n",
        "import csv\n",
        "csv_file_path = '_'.join(desired_keywords) + \"_positive.csv\"\n",
        "with open(csv_file_path, 'w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    for item in positive_list:\n",
        "        writer.writerow([item])\n",
        "csv_file_path = '_'.join(desired_keywords) + \"_negative.csv\"\n",
        "with open(csv_file_path, 'w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    for item in negative_list:\n",
        "        writer.writerow([item])\n",
        "\n",
        "print(len(positive_list))\n",
        "print(len(negative_list))\n",
        "print(\"CSV 파일이 생성되었습니다.\")"
      ],
      "metadata": {
        "id": "s0HqQDLNpfF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 긍정적인 문장들만 따로 모아서 wordcloud생성"
      ],
      "metadata": {
        "id": "GbjRkRGaE88-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# positive_list의 요소들을 공백으로 연결하여 하나의 문자열로 만듦\n",
        "text = ' '.join(positive_list)\n",
        "\n",
        "# WordCloud 생성\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='black').generate(text)\n",
        "\n",
        "# WordCloud를 시각화\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "image_file_name = '_'.join(desired_keywords) + '_positive.png'\n",
        "wordcloud.to_file(image_file_name)\n",
        "print(\"이미지 파일이 생성되었습니다.\")\n"
      ],
      "metadata": {
        "id": "585JUWeS0XpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 부정적인 문장의 wordcloud"
      ],
      "metadata": {
        "id": "r70ZuuqgzzMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "text = ' '.join(negative_list)\n",
        "\n",
        "# WordCloud 생성\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='black').generate(text)\n",
        "\n",
        "# WordCloud를 시각화\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "image_file_name = '_'.join(desired_keywords) + '_negative.png'\n",
        "wordcloud.to_file(image_file_name)\n",
        "print(\"이미지 파일이 생성되었습니다.\")\n"
      ],
      "metadata": {
        "id": "EtN4h4DEzs-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CFpWX5tEFJZF"
      }
    }
  ]
}